---
title: "Estimating Survival (Time-to-Event) Models with rstanarm"
author: "Sam Brilleman"
date: "`r Sys.Date()`"
output: 
  html_vignette:
    toc: true
    number_sections: false
params:
  EVAL: !r identical(Sys.getenv("NOT_CRAN"), "true")
---

<!--
%\VignetteEngine{knitr::rmarkdown}
%\VignetteIndexEntry{stan_surv: Survival (Time-to-Event) Models}
-->

<style type="text/css">
h1 { /* Header 1 */
  font-size: 25px;
}
#TOC { /* Table of contents */
  width: 90%;
}
</style>

```{r, child="children/SETTINGS-knitr.txt"}
```
```{r, child="children/SETTINGS-gg.txt"}
```

```{r setup_jm, include=FALSE, message=FALSE}
knitr::opts_chunk$set(fig.width=10, fig.height=4)
library(rstanarm)
set.seed(989898)
CHAINS <- 1
CORES  <- 1
SEED   <- 12345
ITER   <- 1000
```


# Preamble

This vignette provides an introduction to the `stan_surv` modelling function in the __rstanarm__ package. The `stan_surv` function allows the user to fit survival models (sometimes known as time-to-event models) under a Bayesian framework.


# Introduction

Survival (a.k.a. time-to-event) analysis is generally concerned with the time from some defined baseline (e.g. diagnosis of a disease) until an event of interest occurs (e.g. death or disease progression). 

In standard survival analysis, one event time is measured for each observational unit. In practice however that event time may be unobserved due to left, right, or interval censoring, in which case the event time is only known to have occurred within the relevant censoring interval. 

There are two common approaches to modelling survival data. The first is to model the *rate* of the event (known as the *hazard*) as a function of time -- the class of models known as proportional and non-proportional hazards regression models. The second is to model the event time directly -- the class of models known as accelerated failure time (AFT) models. In addition, a number of extensions to standard survival analysis have been proposed. These include the handling of multiple (recurrent) events, competing events, clustered survival data, cure models, and more.

The intention is for the `stan_surv` modelling function in the **rstanarm** package to provide functionality for fitting a wide range of Bayesian survival models. The current implementation allows for the following model formulations:

- standard parametric (exponential, Weibull and Gompertz) hazard models
- flexible parametric (cubic spline-based) hazard models
- standard parametric (exponential and Weibull) AFT models.

Under each of those model formulations the following are allowed:

- left, right, and interval censored survival data 
- delayed entry (i.e. left truncation)
- covariates can be time-fixed or time-varying (with the latter specified using a "start-stop" data structure)
- coefficients for covariates can be either time-fixed (e.g. proportional hazards) or time-varying (e.g. non-proportional hazards) -- when coefficients are specified as time-varying they can be modelled using either a smooth (cubic) B-spline function or a piecewise constant function.


# Technical details

## Data and notation

We assume that a true event time for individual $i$ ($i = 1,...,N$) exists, denoted $T_i^*$, but that in practice it may or may not observed due to left, right, or interval censoring. Therefore, in practice we observe outcome data $\mathcal{D}_i = \{T_i, T_i^U, T_i^E, d_i\}$ for individual $i$ where:

- $T_i$: the observed event or censoring time
- $T_i^U$: the observed upper limit for interval censored individuals
- $T_i^E$: the observed entry time (i.e. the time at which an individual became at risk for the event)

and $d_i \in \{0,1,2,3\}$ denotes an event indicator taking value:

- 0 if individual $i$ was right censored (i.e. $T_i^* > T_i$) 
- 1 if individual $i$ was uncensored (i.e. $T_i^* = T_i$)
- 2 if individual $i$ was left censored (i.e. $T_i^* < T_i$)
- 3 if individual $i$ was interval censored (i.e. $T_i < T_i^* < T_i^U$)

## The hazard rate, cumulative hazard, and survival probability

The hazard of the event at time $t$ is the instantaneous rate of occurrence for the event at time $t$. Mathematically, it is defined as:
\
\begin{equation}
\begin{split}
h_i(t) = \lim_{\Delta t \to 0} 
  \frac{P(t \leq T_i^* < t + \Delta t | T_i^* > t)}{\Delta t}
\end{split}
\end{equation}
\
where $\Delta t$ is the width of some small time interval. The numerator is the conditional probability of the individual experiencing the event during the time interval $[t, t + \Delta t)$, given that they were still at risk of the event at time $t$. The denominator converts the conditional probability to a rate per unit of time. As $\Delta t$ approaches the limit, the width of the interval approaches zero and the instantaneous event rate is obtained.

The cumulative hazard is defined as:
\
\begin{equation}
\begin{split}
H_i(t) = \int_{s=0}^t h_i(s) ds
\end{split}
\end{equation}
\
and the survival probability is defined as:
\
\begin{equation}
\begin{split}
S_i(t) = \exp \left[ -H_i(t) \right] = \exp \left[ -\int_{s=0}^t h_i(s) ds \right]
\end{split}
\end{equation}

It can be seen here that in the standard survival analysis setting there is a one-to-one relationship between each of the hazard, the cumulative hazard, and the survival probability. These quantities are also used to form the likelihood for the survival model described in the later sections.

## Hazard scale formulations

When `basehaz` is set equal to `"exp"`, `"weibull"`, `"gompertz"`, `"ms"` (the default), or `"bs"` then the model is defined on the hazard scale as described by the following parameterisations. 

We model the hazard of the event for individual $i$ at time $t$ using the regression model:
\
\begin{equation}
\begin{split}
h_i(t) = h_0(t) \exp \left[ \eta_i(t) \right]
\end{split}
\end{equation}
\
where $h_0(t)$ is the baseline hazard (i.e. the hazard for an individual with all covariates set equal to zero) at time $t$, and $\eta_i(t)$ denotes the linear predictor evaluated for individual $i$ at time $t$. For full generality, we allow the linear predictor to be time-varying; that is, it may be a function of time-varying covariates or time-varying coefficients (i.e. a time-varying hazard ratio). However, if there are no time-varying covariates or time-varying coefficients in the model, then the linear predictor reduces to a time-fixed quantity and the definition of the hazard function reduces to:
\
\begin{equation}
\begin{split}
h_i(t) = h_0(t) \exp \left[ \eta_i \right]
\end{split}
\end{equation}

Our linear predictor is defined as:
\
\begin{equation}
\begin{split}
\eta_i(t) = \beta_0 + \sum_{p=1}^P \beta_p(t) x_{ip}(t)
\end{split}
\end{equation}
\
where $\beta_0$ denotes the intercept parameter, $x_{ip}(t)$ denotes the observed value of $p^{th}$ $(p=1,...,P)$ covariate for the $i^{th}$ $(i=1,...,N)$ individual at time $t$, and $\beta_p(t)$ denotes the coefficient for the $p^{th}$ covariate.

The quantity $\exp \left( \beta_p(t) \right)$ is referred to as a "hazard ratio". The *hazard ratio (HR)* quantifies the relative increase in the hazard that is associated with a unit-increase in the relevant covariate, $x_{ip}$; e.g. a hazard ratio of 2 means that a unit-increase in the covariate leads to a doubling in the hazard (i.e. the instantaneous rate) of the event. The hazard ratio can be treated as a time-fixed quantity (i.e. proportional hazards) or time-varying quantity (i.e. non-proportional hazards), as described in later sections.

### Distributions

- **Exponential model** (`basehaz = "exp"`): for scale parameter $\lambda_i(t) = \exp ( \eta_i(t) )$ we have:
\
\begin{equation}
h_i(t) = \lambda_i(t)
\end{equation}

- **Weibull model** (`basehaz = "weibull"`): for scale parameter $\lambda_i(t) = \exp ( \eta_i(t) )$ and shape parameter $\gamma > 0$ we have:
\
\begin{equation}
h_i(t) = \gamma t^{\gamma-1} \lambda_i(t)
\end{equation}

- **Gompertz model** (`basehaz = "gompertz"`): for shape parameter $\lambda_i(t) = \exp ( \eta_i(t) )$ and scale parameter $\gamma > 0$ we have:
\
\begin{equation}
h_i(t) = \exp(\gamma t) \lambda_i(t)
\end{equation}

- **M-splines model** (`basehaz = "ms"`, the default): letting $M(t; \boldsymbol{\gamma}, \boldsymbol{k_0}, \delta)$ denote a degree $\delta$ M-spline function with basis evaluated at a vector of knot locations $\boldsymbol{k_0} = $ and parameter vector $\boldsymbol{\gamma} > 0$ we have:
\
\begin{equation}
h_i(t) = M(t; \boldsymbol{\gamma}, \boldsymbol{k_0}, \delta) \exp ( \eta_i(t) )
\end{equation}
\
The M-spline function is calculated using the method described in Ramsay (1988) and implemented in the **splines2** R package (Wang and Yan (2018)). To ensure that the hazard function $h_i(t)$ is not constrained to zero at the origin (i.e. when $t$ approaches 0) the M-spline basis incorporates an intercept. To ensure identifiability of both the intercept parameter in the M-spline function and the intercept parameter in the linear predictor (i.e. $\beta_0$) we constrain the M-spline coefficients to a simplex, that is, $\sum_{j=1}^J{\gamma_j} = 1$. The default degree in **rstanarm** is $\delta = 3$; that is, cubic M-splines. However this can be controlled by the user via the `basehaz_ops` argument. It is worthwhile noting that $\delta = 0$ would correspond to a piecewise constant baseline hazard.

- **B-splines model** (for the *log* baseline hazard): letting $B(t; \boldsymbol{\gamma}, \boldsymbol{k_0}, \delta)$ denote a degree $\delta$ B-spline function with basis evaluated at a vector of knot locations $\boldsymbol{k_0}$ and parameter vector $\boldsymbol{\gamma}$ we have:
\
\begin{equation}
h_i(t) = \exp ( B(t; \boldsymbol{\gamma}, \boldsymbol{k_0}, \delta) + \eta_i(t) )
\end{equation}
\
The B-spline function is calculated using the method implemented in the **splines2** R package (Wang and Yan (2018)). The B-spline basis does not require an intercept and therefore does not include one; any constant shift in the log hazard is fully captured via the intercept in the linear predictor (i.e. $\beta_0$).

**Note:** When the linear predictor *is not* time-varying (i.e. under proportional hazards) there is a closed form expression for the survival probability (except for the B-splines model); details shown in the appendix. However, when the linear predictor *is* time-varying (i.e. under non-proportional hazards) there is no closed form expression for the survival probability; instead, quadrature is used to evaluate the survival probability for inclusion in the likelihood. Extended details on the parameterisations are given in the appendix.

## Accelerated failure time formulations

When `basehaz` is set equal to `"exp-aft"`, or `"weibull-aft"` then the model is defined on the accelerated failure time scale as described by the following parameterisations. 

Following Hougaard (1999), we model the survival probability for individual $i$ at time $t$ using the regression model:
\
\begin{equation}
\begin{split}
S_i(t) = S_0 \left( \int_0^t \exp \left[ - \eta_i(u) \right] du \right)
\end{split}
\end{equation}
\
where $S_0(t)$ is the baseline survival probability at time $t$, and $\eta_i(t)$ denotes the linear predictor evaluated for individual $i$ at time $t$. For full generality, we allow the linear predictor to be time-varying; that is, it may be a function of time-varying covariates or time-varying coefficients (i.e. a time-varying acceleration factor). However, if there are no time-varying covariates or time-varying coefficients in the model, then the linear predictor reduces to a time-fixed quantity (i.e. $\eta_i(t) = \eta_i$) and the definition of the survival probability reduces to:
\
\begin{equation}
\begin{split}
S_i(t) = S_0 \left( t \exp \left[ - \eta_i \right] \right)
\end{split}
\end{equation}

Our linear predictor is defined as:
\
\begin{equation}
\begin{split}
\eta_i(t) = \beta_0^* + \sum_{p=1}^P \beta_p^*(t) x_{ip}(t)
\end{split}
\end{equation}
\
where $\beta_0^*$ denotes the intercept parameter, $x_{ip}(t)$ denotes the observed value of $p^{th}$ $(p=1,...,P)$ covariate for the $i^{th}$ $(i=1,...,N)$ individual at time $t$, and $\beta_p^*(t)$ denotes the coefficient for the $p^{th}$ covariate.

The quantity $\exp \left( - \beta_p^*(t) \right)$ is referred to as an "acceleration factor" and the quantity $\exp \left( \beta_p^*(t) \right)$ is referred to as a "survival time ratio". The *acceleration factor* (AF) quantifies the acceleration (or deceleration) of the event process that is associated with a unit-increase in the relevant covariate, $x_{ip}$; e.g. an acceleration factor of 0.5 means that a unit-increase in the covariate leads to an individual approaching the event at half the speed. If you find that somewhat confusing, then it may be easier to think about the survival time ratio. The *survival time ratio* (STR) is interpreted as the increase (or decrease) in the expected survival time that is associated with a unit-increase in the relevant covariate, $x_{ip}$; e.g. a survival time ratio of 2 (which is equivalent to an acceleration factor of 0.5) means that a unit-increase in the covariate leads to an doubling in the expected survival time. The survival time ratio is equal to the inverse of the acceleration factor (i.e. $\text{STR} = 1/\text{AF}$).

### Distributions

- **Exponential model** (`basehaz = "exp-aft"`): When the linear predictor is time-varying we have:
\
\begin{equation}
S_i(t) = \exp \left( - \int_0^t \exp ( -\eta_i(u) ) du \right)
\end{equation}
\
and when the linear predictor is time-fixed we have:
\
\begin{equation}
S_i(t) = \exp \left( - t \lambda_i \right)
\end{equation}
\
for scale parameter $\lambda_i = \exp ( -\eta_i )$.

- **Weibull model** (`basehaz = "weibull-aft"`): When the linear predictor is time-varying we have:
\
\begin{equation}
S_i(t) = \exp \left( - \left[ \int_0^t \exp ( -\eta_i(u) ) du \right]^{\gamma} \right)
\end{equation}
\
for shape parameter $\gamma > 0$ and when the linear predictor is time-fixed we have:
\
\begin{equation}
S_i(t) = \exp \left( - t^{\gamma} \lambda_i \right)
\end{equation}
\
for scale parameter $\lambda_i = \exp ( -\gamma \eta_i )$ and shape parameter $\gamma > 0$.

**Note:** When the linear predictor *is not* time-varying (i.e. under time-fixed acceleration), there is a closed form expression for both the hazard function and survival function; details shown in the appendix. However, when the linear predictor *is* time-varying (i.e. under time-varying acceleration) there is no closed form expression for the hazard function or survival probability; instead, quadrature is used to evaluate the cumulative acceleration factor, which in turn is used to evaluate the hazard function and survival probability for inclusion in the likelihood. Extended details on the parameterisations are given in the appendix.

## Time-fixed and time-varying effects of covariates

The coefficient $\beta_p(t)$ (i.e. the log hazard ratio) or $\beta_p^*(t)$ (i.e. log survival time ratio) can be treated as a time-fixed quantity (e.g. $\beta_p(t) = \beta_p$) or as a time-varying quantity. We refer to the latter as *time-varying effects* because the effect of the covariate is allowed to change as a function of time. In `stan_surv` time-varying effects are specified by using the `tve` function in the model formula. Note that in the following definitions we only refer to $\beta_p(t)$ (i.e. the log hazard ratio) but the same methodology applies to $\beta_p^*(t)$ (i.e. the log survival time ratio).

Without time-varying effects we have:
\
\begin{equation}
\begin{split}
\beta_p(t) = \theta_{p0}
\end{split}
\end{equation}
\
such that $\theta_{p0}$ is a time-fixed log hazard ratio (or log survival time ratio).

With **time-varying effects modelled using B-splines** we have:
\
\begin{equation}
\begin{split}
\beta_p(t) = \theta_{p0} + \sum_{m=1}^{M} \theta_{pm} B_{m}(t; \boldsymbol{k}, \delta)
\end{split}
\end{equation}
\
where $\theta_{p0}$ is a constant, $B_{m}(t; \boldsymbol{k}, \delta)$ is the $m^{\text{th}}$ $(m = 1,...,M)$ basis term for a degree $\delta$ B-spline function evaluated at a vector of knot locations $\boldsymbol{k} = \{k_{1},...,k_{J}\}$, and $\theta_{pm}$ is the $m^{\text{th}}$ B-spline coefficient. By default cubic B-splines are used (i.e. $\delta = 3$). These allow the log hazard ratio (or log survival time ratio) to be modelled as a smooth function of time. 

The degrees of freedom is equal to the number of additional parameters required to estimate a time-varying coefficient relative to a time-fixed coefficient. When a B-spline function is used to model the time-varying coefficient the degrees of freedom are $M = J + \delta - 2$ where $J$ is the total number of knots (including boundary knots). 

With **time-varying effects modelled using a piecewise constant function** we have:
\
\begin{equation}
\begin{split}
\beta_p(t) = \theta_{p0} + \sum_{m=1}^{M} \theta_{pm} I(k_{m+1} < t \leq k_{m+2})
\end{split}
\end{equation}
\
where $I(x)$ is an indicator function taking value 1 if $x$ is true and 0 otherwise, $\theta_{p0}$ is a constant corresponding to the log hazard ratio (or log survival time ratio for AFT models) in the first time interval, $\theta_{pm}$ is the deviation in the log hazard ratio (or log survival time ratio) between the first and $(m+1)^\text{th}$ $(m = 1,...,M)$ time interval, and $\boldsymbol{k} = \{k_{1},...,k_{J}\}$ is a sequence of knot locations (i.e. break points) that includes the lower and upper boundary knots. This allows the log hazard ratio (or log survival time ratio) to be modelled as a piecewise constant function of time.

The degrees of freedom is equal to the number of additional parameters required to estimate a time-varying coefficient relative to a time-fixed coefficient. When a piecewise constant function is used to model the time-varying coefficient the degrees of freedom are $M = J - 2$ where $J$ is the total number of knots (including boundary knots).

**Default knot locations:** The vector of knot locations $\boldsymbol{k} = \{k_{1},...,k_{J}\}$ includes a lower boundary knot $k_{1}$ at the earliest entry time (equal to zero if there isn't delayed entry) and an upper boundary knot $k_{J}$ at the latest event or censoring time. The boundary knots cannot be changed by the user. Internal knot locations -- that is $k_{2},...,k_{(J-1)}$ when $J \geq 3$ -- can be explicitly specified by the user (see the `knots` argument to the `tve` function) or are determined by default. The default is to place the internal knots at equally spaced percentiles of the distribution of uncensored event times. When a B-spline function is specified, the `tve` function uses default values $M = 3$ (degrees of freedom) and $\delta = 3$ (cubic splines) which in fact corresponds to a cubic B-spline function with no internal knots. When a piecewise constant function is specified, the `tve` function uses a default value of $M = 3$ (degrees of freedom) which corresponds to internal knots at the $25^{\text{th}}$, $50^{\text{th}}$, and $75^{\text{th}}$ percentiles of the distribution of the uncensored event times.

**Note on subscripts:** We have dropped the subscript $p$ from the knot locations $\boldsymbol{k}$ and degree of the B-splines $\delta$ discussed above. This is just for simplicity of the notation. In fact, if a model has time-varying effects estimated for more than one covariate, then each these can be modelled using different knot locations and/or degree if the user desires.

## Likelihood

Allowing for the three forms of censoring and potential delayed entry (i.e. left truncation) the likelihood for the survival model takes the form:
\
\begin{align}
\begin{split}
p(\mathcal{D}_i | \boldsymbol{\gamma}, \boldsymbol{\beta}) =
  &        {\left[ h_i(T_i)              \right]}^{I(d_i=1)} \\
  & \times {\left[ S_i(T_i)              \right]}^{I(d_i \in \{0,1\})} \\
  & \times {\left[ 1 - S_i(T_i)          \right]}^{I(d_i=2)} \\
  & \times {\left[ S_i(T_i) - S_i(T_i^U) \right]}^{I(d_i=3)} \\
  & \times {\left[ S_i(T_i^E)            \right]}^{-1}
\end{split}
\end{align}

## Priors

The prior distribution for the so-called "auxiliary" parameters (i.e. $\gamma$ for the Weibull and Gompertz models, or $\boldsymbol{\gamma}$ for the M-spline and B-spline models) is specified via the `prior_aux` argument to `stan_surv`. Choices of prior distribution include:

- a Dirichlet prior is allowed for the M-spline coefficients $\boldsymbol{\gamma}$
- a half-normal, half-t, half-Cauchy or exponential prior is allowed for the Weibull shape parameter $\gamma$
- a half-normal, half-t, half-Cauchy or exponential prior is allowed for the Gompertz scale parameter $\gamma$
- a normal, t, or Cauchy prior is allowed for the B-spline coefficients $\boldsymbol{\gamma}$

These choices are described in greater detail in the `stan_surv` or `priors` help file.

The prior distribution for the intercept parameter in the linear predictor is specified via the `prior_intercept` argument to `stan_surv`. Choices include the normal, t, or Cauchy distributions. The default is a normal distribution with mean zero and scale 20. Note that -- internally (but not in the reported parameter estimates) -- the prior is placed on the intercept *after* centering the predictors at their sample means and *after* applying a constant shift of $\log \left( \frac{E}{T} \right)$ where $E$ is the total number of events and $T$ is the total follow up time. For example, a prior specified by the user as `prior_intercept = normal(0,20)` is in fact not centered on an intercept of zero when all predictors are at their sample means, but rather, it is centered on the log crude event rate when all predictors are at their means. This is intended to help with numerical stability and sampling, but does not impact on the reported estimates (i.e. the intercept is back-transformed before being returned to the user).

The choice of prior distribution for the time-fixed coefficients $\theta_{p0}$ ($p = 1,...,P$) is specified via the `prior` argument to `stan_surv`. This can any of the standard prior distributions allowed for regression coefficients in the **rstanarm** package; see the [priors vignette](priors.html) and the `stan_surv` help file for details. 

The additional coefficients required for estimating time-varying effects (i.e. the B-spline coefficients or the interval-specific deviations in the piecewise constant function) are given a random walk prior of the form $\theta_{p,1} \sim N(0,1)$ and $\theta_{p,m} \sim N(\theta_{p,m-1},\tau_p)$ for $m = 2,...,M$, where $M$ is the total number of cubic B-spline basis terms. The prior distribution for the hyperparameter $\tau_p$ is specified via the `prior_smooth` argument to `stan_surv`. Lower values of $\tau_p$ lead to a less flexible (i.e. smoother) function. Choices of prior distribution for the hyperparameter $\tau_p$ include an exponential, half-normal, half-t, or half-Cauchy distribution, and these are detailed in the `stan_surv` help file.


# Usage examples

## Example: A flexible parametric proportional hazards model

We will use the German Breast Cancer Study Group dataset (see `?rstanarm-datasets` for details and references). In brief, the data consist of
$N = 686$ patients with primary node positive breast cancer recruited between 1984-1989. The primary response is time to recurrence or death. Median follow-up time was 1084 days. Overall, there were 299 (44%) events and the remaining 387 (56%) individuals were right censored. We concern our analysis here with a 3-category baseline covariate for cancer prognosis (good/medium/poor).

First, let us load the data and fit the proportional hazards model

```{r, warning = FALSE, message = FALSE, results='hide'}
mod1 <- stan_surv(formula = Surv(recyrs, status) ~ group, 
                  data    = bcancer, 
                  chains  = CHAINS, 
                  cores   = CORES, 
                  seed    = SEED,
                  iter    = ITER)
```

The model here is estimated using the default cubic M-splines (with 5 degrees of freedom) for modelling the baseline hazard. Since there are no time-varying effects in the model (i.e. we did not wrap any covariates in the `tve()` function) there is a closed form expression for the cumulative hazard and survival function and so the model is relatively fast to fit. Specifically, the model takes ~3.5 sec for each MCMC chain based on the default 2000 (1000 warm up, 1000 sampling) MCMC iterations. 

We can easily obtain the estimated hazard ratios for the 3-catgeory group covariate using the generic `print` method for `stansurv` objects, as follows

```{r}
print(mod1, digits = 3)
```

We see from this output we see that individuals in the groups with `Poor` or `Medium` prognosis have much higher rates of death relative to the group with `Good` prognosis (as we might expect!). The hazard of death in the `Poor` prognosis group is approximately 5.0-fold higher than the hazard of death in the `Good` prognosis group. Similarly, the hazard of death in the `Medium` prognosis group is approximately 2.3-fold higher than the hazard of death in the `Good` prognosis group.

It may also be of interest to compare the different types of the baseline hazard we could potentially use. Here, we will fit a series of models, each with a different baseline hazard specification 

```{r, warning = FALSE, message = FALSE, results='hide'}
mod1_exp      <- update(mod1, basehaz = "exp")
mod1_weibull  <- update(mod1, basehaz = "weibull")
mod1_gompertz <- update(mod1, basehaz = "gompertz")
mod1_bspline  <- update(mod1, basehaz = "bs")
mod1_mspline1 <- update(mod1, basehaz = "ms")
mod1_mspline2 <- update(mod1, basehaz = "ms", basehaz_ops = list(df = 10))
```

and then plot the baseline hazards with 95% posterior uncertainty limits using the generic `plot` method for `stansurv` objects (note that the default `plot` for `stansurv` objects is the estimated baseline hazard). We will write a little helper function to adjust the y-axis limits, add a title, and centre the title, on each plot, as follows

```{r, fig.height=5}
library(ggplot2)
plotfun <- function(model, title) {
  plot(model, plotfun = "basehaz") +              # plot baseline hazard
    coord_cartesian(ylim = c(0,0.4)) +            # adjust y-axis limits
    labs(title = title) +                         # add plot title
    theme(plot.title = element_text(hjust = 0.5)) # centre plot title
}
p_exp      <- plotfun(mod1_exp,      title = "Exponential")
p_weibull  <- plotfun(mod1_weibull,  title = "Weibull")
p_gompertz <- plotfun(mod1_gompertz, title = "Gompertz")
p_bspline  <- plotfun(mod1_bspline,  title = "B-splines with df = 5")
p_mspline1 <- plotfun(mod1_mspline1, title = "M-splines with df = 5")
p_mspline2 <- plotfun(mod1_mspline2, title = "M-splines with df = 10")
bayesplot::bayesplot_grid(p_exp,
                          p_weibull,
                          p_gompertz,
                          p_bspline,
                          p_mspline1,
                          p_mspline2,
                          grid_args = list(ncol = 3))
```

We can also compare the fit of these models using the `loo` method for `stansurv` objects

```{r, message=FALSE}
loo_compare(loo(mod1_exp),
            loo(mod1_weibull),
            loo(mod1_gompertz),
            loo(mod1_bspline),
            loo(mod1_mspline1),
            loo(mod1_mspline2))
```

where we see that models with a flexible parametric (spline-based) baseline hazard fit the data best followed by the standard parametric (Weibull, Gompertz, exponential) models. Roughly speaking, the B-spline and M-spline models seem to fit the data equally well since the differences in `elpd` or `looic` between the models are very small relative to their standard errors. Moreover, increasing the degrees of freedom for the M-splines from 5 to 10 doesn't seem to improve the fit (that is, the default degrees of freedom `df = 5` seems to provide sufficient flexibility to model the baseline hazard).

After fitting the survival model, we often want to estimate the predicted survival function for individual's with different covariate patterns. Here, let us estimate the predicted survival function between 0 and 5 years for an individual in each of the prognostic groups. To do this, we can use the `posterior_survfit` method for `stansurv` objects, and it's associated `plot` method. First let us construct the prediction (covariate) data

```{r preddata}
nd <- data.frame(group = c("Good", "Medium", "Poor"))
head(nd)
```

and then we will generate the posterior predictions 

```{r predresults}
ps <- posterior_survfit(mod1, newdata = nd, times = 0, extrapolate = TRUE,
                        control = list(edist = 5))
head(ps)
```

Here we note that the `id` variable in the data frame of posterior predictions identifies which row of `newdata` the predictions correspond to. For demonstration purposes we have also shown a couple of other arguments in the `posterior_survfit` call, namely

- the `times = 0` argument says that we want to predict at time = 0 (i.e. baseline) for each individual in the `newdata` (this is the default anyway)
- the `extrapolate = TRUE` argument says that we want to extrapolate forward from time 0 (this is also the default)
- the `control = list(edist = 5)` identifies the control of the extrapolation; this is saying extrapolate the survival function forward from time 0 for a distance of 5 time units (the default would have been to extrapolate as far as the largest event or censoring time in the estimation dataset, which is 7.28 years in the `brcancer` data).

Let us now plot the survival predictions. We will relabel the `id` variable with meaningful labels identifying the covariate profile of each new individual in our prediction data

```{r predplot}
panel_labels <- c('1' = "Good", '2' = "Medium", '3' = "Poor")
plot(ps) + 
  ggplot2::facet_wrap(~ id, labeller = ggplot2::labeller(id = panel_labels))
```

We can see from the plot that predicted survival is worst for patients with a `Poor` diagnosis, and best for patients with a `Good` diagnosis, as we would expect based on our previous model estimates.

Alternatively, if we wanted to obtain and plot the predicted *hazard* function for each individual in our new data (instead of their *survival* function), then we just need to specify `type = "haz"` in our `posterior_survfit` call (the default is `type = "surv"`), as follows

```{r predhaz}
ph <- posterior_survfit(mod1, newdata = nd, type = "haz")
plot(ph) + 
  ggplot2::facet_wrap(~ id, labeller = ggplot2::labeller(id = panel_labels))
```

We can quite clearly see in the plot the assumption of proportional hazards. We can also see that the hazard is highest in the `Poor` prognosis group (i.e. worst survival) and the hazard is lowest in the `Good` prognosis group (i.e. best survival). This corresponds to what we saw in the plot of the survival functions previously.

## Example: Non-proportional hazards modelled using B-splines

To demonstrate the implementation of time-varying effects in `stan_surv` we will use a simulated dataset, generated using the **simsurv** package (Brilleman, 2018).

We will simulate a dataset with $N = 200$ individuals with event times generated under the following Weibull hazard function
\
\begin{align}
h_i(t) = \gamma t^{\gamma-1} \lambda \exp( \beta(t) x_i )
\end{align}
\
with scale parameter $\lambda = 0.1$, shape parameter $\gamma = 1.5$, binary baseline covariate $X_i \sim \text{Bern}(0.5)$, and time-varying hazard ratio $\beta(t) = -0.5 + 0.2 t$. We will enforce administrative censoring at 5 years if an individual's simulated event time is >5 years.

```{r simsurv-simdata}
# load package
library(simsurv)
# set seed for reproducibility
set.seed(999111)
# simulate covariate data
covs <- data.frame(id  = 1:200, 
                   trt = rbinom(200, 1L, 0.5))
# simulate event times
dat  <- simsurv(lambdas = 0.1, 
                gammas  = 1.5, 
                betas   = c(trt = -0.5),
                tde     = c(trt = 0.2),
                x       = covs, 
                maxt    = 5)
# merge covariate data and event times
dat  <- merge(dat, covs)
# examine first few rows of data
head(dat)
```

Now that we have our simulated dataset, let us fit a model with time-varying hazard ratio for `trt`

```{r tve_fit1, warning = FALSE, message = FALSE, results='hide'}
mod2 <- stan_surv(formula = Surv(eventtime, status) ~ tve(trt), 
                  data    = dat, 
                  chains  = CHAINS, 
                  cores   = CORES, 
                  seed    = SEED,
                  iter    = ITER)
```

The `tve` function is used in the model formula to state that we want a time-varying effect (i.e. a time-varying coefficient) to be estimated for the variable `trt`. By default, a cubic B-spline basis with 3 degrees of freedom (i.e. two boundary knots placed at the limits of the range of event times, but no internal knots) is used for modelling the time-varying log hazard ratio. If we wanted to change the degree, knot locations, or degrees of freedom for the B-spline function we can specify additional arguments to the `tve` function. 

For example, to model the time-varying log hazard ratio using quadratic B-splines with 4 degrees of freedom (i.e. two boundary knots placed at the limits of the range of event times, as well as two internal knots placed -- by default -- at the 33.3rd and 66.6th percentiles of the distribution of uncensored event times) we could specify the model formula as

```{r, warning = FALSE, message = FALSE, results='hide', eval=FALSE}
Surv(eventtime, status) ~ tve(trt, df = 4, degree = 2)
```

Let us now plot the estimated time-varying hazard ratio from the fitted model. We can do this using the generic `plot` method for `stansurv` objects, for which we can specify the `plotfun = "tve"` argument. (Note that in this case, there is only one covariate in the model with a time-varying effect, but if there were others, we could specify which covariate(s) we want to plot the time-varying effect for by specifying the `pars` argument to the `plot` call).

```{r, fig.height=5}
plot(mod2, plotfun = "tve")
```

From the plot, we can see how the hazard ratio (i.e. the effect of treatment on the hazard of the event) changes as a function of time. The treatment appears to be protective during the first few years following baseline (i.e. HR < 1), and then the treatment appears to become harmful after about 4 years post-baseline. Thankfully, this is a reflection of the model we simulated under! 

The plot shows a large amount of uncertainty around the estimated time-varying hazard ratio. This is to be expected, since we only simulated a dataset of 200 individuals of which only around 70% experienced the event before being censored at 5 years. So, there is very little data (i.e. very few events) with which to reliably estimate the time-varying hazard ratio. We can also see this reflected in the differences between our data generating model and the estimates from our fitted model. In our data generating model, the time-varying hazard ratio equals 1 (i.e. the log hazard ratio equals 0) at 2.5 years, but in our fitted model the median estimate for our time-varying hazard ratio equals 1 at around ~3 years. This is a reflection of the large amount of sampling error, due to our simulated dataset being so small.

## Example: Non-proportional hazards modelled using a piecewise constant function

In the previous example we showed how non-proportional hazards can be modelled by using a smooth B-spline function for the time-varying log hazard ratio. This is the default approach when the `tve` function is used to estimate a time-varying effect for a covariate in the model formula. However, another approach for modelling a time-varying log hazard ratio is to use a piecewise constant function. If we want to use a piecewise constant for the time-varying log hazard ratio (instead of the smooth B-spline function) then we just have to specify the `type` argument to the `tve` function.

We will again simulate some survival data using the **simsurv** package to show how a piecewise constant hazard ratio can be estimated using `stan_surv`.

Similar to the previous example, we will simulate a dataset with $N = 500$ individuals with event times generated under a Weibull hazard function with scale parameter $\lambda = 0.1$, shape parameter $\gamma = 1.5$, and binary baseline covariate $X_i \sim \text{Bern}(0.5)$. However, in this example our time-varying hazard ratio will be defined as $\beta(t) = -0.5 + 0.7 \times I(t > 2.5)$ where $I(X)$ is the indicator function taking the value 1 if $X$ is true and 0 otherwise. This corresponds to a piecewise constant log hazard ratio with just two "pieces" or time intervals. The first time interval is $[0,2.5]$ during which the true hazard ratio is $\exp(-0.5) = 0.61$. The second time interval is $(2.5,\infty]$ during which the true log hazard ratio is $\exp(-0.5 + 0.7) = 1.22$. Our example uses only two time intervals for simplicity, but in general we could easily have considered more (although it would have required couple of additional lines of code to simulate the data). We will again enforce administrative censoring at 5 years if an individual's simulated event time is >5 years.

```{r simsurv-simdata2}
# load package
library(simsurv)
# set seed for reproducibility
set.seed(888222)
# simulate covariate data
covs <- data.frame(id  = 1:500, 
                   trt = rbinom(500, 1L, 0.5))
# simulate event times
dat  <- simsurv(lambdas = 0.1, 
                gammas  = 1.5, 
                betas   = c(trt = -0.5),
                tde     = c(trt = 0.7),
                tdefun  = function(t) (t > 2.5),
                x       = covs, 
                maxt    = 5)
# merge covariate data and event times
dat  <- merge(dat, covs)
# examine first few rows of data
head(dat)
```

We now estimate a model with a piecewise constant time-varying effect for the covariate `trt` as

```{r tve-fit2, warning = FALSE, message = FALSE, results='hide'}
mod3 <- stan_surv(formula = Surv(eventtime, status) ~ 
                    tve(trt, degree = 0, knots = 2.5),
                  data    = dat, 
                  chains  = CHAINS, 
                  cores   = CORES, 
                  seed    = SEED,
                  iter    = ITER)
```

This time we specify some additional arguments to the `tve` function, so that our time-varying effect corresponds to the true data generating model used to simulate our event times. Specifically, we specify `degree = 0` to say that we want the time-varying effect (i.e. the time-varying log hazard ratio) to be estimated using a piecewise constant function and `knots = 2.5` says that we only want one internal knot placed at the time $t = 2.5$.

We can again use the generic `plot` function with argument `plotfun = "tve"` to examine our estimated hazard ratio for treatment

```{r, fig.height=5}
plot(mod3, plotfun = "tve")
```

Here we see that the estimated hazard ratio reasonably reflects our true data generating model (i.e. a hazard ratio of $\approx 0.6$ during the first time interval and a hazard ratio of $\approx 1.2$ during the second time interval) although there is a slight discrepancy due to the sampling variation in the simulated event times.

## Example: Hierarchical survival models

To demonstrate the estimation of a hierarchical model for survival data in `stan_surv` we will use the `frail` dataset (see `help("rstanarm-datasets")` for a description). The `frail` datasets contains simulated event times for 200 patients clustered within 20 hospital sites (10 patients per hospital site). The event times are simulated from a parametric proportional hazards model under the following assumptions: (i) a constant (i.e. exponential) baseline hazard rate of 0.1; (ii) a fixed treatment effect with log hazard ratio of 0.3; and (iii) a site-specific random intercept (specified on the log hazard scale) drawn from a N(0,1) distribution.

Let's look at the first few rows of the data:

```{r frail-data-head}
head(frail)
```

To fit a hierarchical model for clustered survival data we use a formula syntax similar to what is used in the **lme4** R package (Bates et al. (2015)). Let's consider the following model (which aligns with the model used to generate the simulated data):

```{r frail-fit-model, warning = FALSE, message = FALSE}
mod_randint <- stan_surv(
  formula = Surv(eventtime, status) ~ trt + (1 | site),
  data    = frail,
  basehaz = "exp",
  chains  = CHAINS, 
  cores   = CORES, 
  seed    = SEED,
  iter    = ITER)
```

The model contains a baseline covariate for treatment (0 or 1) as well as a site-specific intercept to allow for correlation in the event times for patients from the same site. We've call the model object `mod_randint` to denote the fact that it includes a site-specific (random) intercept. Let's examine the parameter estimates from the model:

```{r frail-estimates}
print(mod_randint, digits = 2)
```

We see that the estimated log hazard ratio for treatment ($\hat{\beta}_{\text{(trt)}} = 0.46$) is a bit larger than the "true" log hazard ratio used in the data generating model ($\beta_{\text{(trt)}} = 0.3$). The estimated baseline hazard rate is $\exp(-2.3716) = 0.093$, which is pretty close to the baseline hazard rate used in the data generating model ($0.1$). Of course, the differences between the estimated parameters and the true parameters from the data generating model are attributable to sampling noise.

If this were a real analysis, we might wonder whether the site-specific estimates are necessary! Well, we can assess that by fitting an alternative model that does **not** include the site-specific intercepts and compare it to the model we just estimated. We will compare it using the `loo` function. We first need to fit the model without the site-specific intercept. To do this, we will just use the generic `update` method for `stansurv` objects, since all we are changing is the model formula:

```{r frail-fixed-model, warning = FALSE, message = FALSE}
mod_fixed <- update(mod_randint, formula. = Surv(eventtime, status) ~ trt) 
```

Let's calculate the `loo` for both these models and compare them:

```{r frail-compare-1, warning = FALSE, message = FALSE}
loo_fixed   <- loo(mod_fixed)
loo_randint <- loo(mod_randint)
loo_compare(loo_fixed, loo_randint)
```

We see strong evidence in favour of the model with the site-specific intercepts!

But let's not quite finish there. What about if we want to generalise the random effects structure further. For instance, is the site-specific intercept enough? Perhaps we should consider estimating both a site-specific intercept and a site-specific treatment effect. We have minimal data to estimate such a model (recall that there is only 20 sites and 10 patients per site) but for the sake of demonstration we will forge on nonetheless. Let's fit a model with both a site-specific intercept and a site-specific coefficient for the covariate `trt` (i.e. treatment):

```{r frail-random-trt, warning = FALSE, message = FALSE}
mod_randtrt <- update(mod_randint, formula. = 
                        Surv(eventtime, status) ~ trt + (trt | site)) 
print(mod_randtrt, digits = 2)
```

We see that we have an estimated standard deviation for the site-specific intercepts and the site-specific coefficients for `trt`, as well as the estimated correlation between those site-specific parameters.

Let's now compare all three of these models based on `loo`:

```{r frail-compare-2, warning = FALSE, message = FALSE}
loo_randtrt <- loo(mod_randtrt)
loo_compare(loo_fixed, loo_randint, loo_randtrt)
```

It appears that the model with just a site-specific intercept is the best fitting model. It is much better than the model without a site-specific intercept, and slightly better than the model with both a site-specific intercept and a site-specific treatment effect. In other words, including a site-specific intercept appears important, but including a site-specific treatment effect is not. This conclusion is reassuring, because it aligns with the data generating model we used to simulate the data!


# References

Bates D, Maechler M, Bolker B, Walker S. Fitting Linear Mixed-Effects Models Using lme4. *Journal of Statistical Software* 2015;67(1):1--48. \url{https://doi.org/10.18637/jss.v067.i01}

Brilleman S. (2018) *simsurv: Simulate Survival Data.* R package version 0.2.2. \url{https://CRAN.R-project.org/package=simsurv}

Hougaard P. Fundamentals of Survival Data. *Biometrics* 1999;55:13--22.

Ramsay JO. Monotone Regression Splines in Action. *Statistical Science* 1988;3(4):425--461. \url{https://doi.org/10.1214/ss/1177012761}

Wang W, Yan J. (2018) *splines2: Regression Spline Functions and Classes.* R package version 0.2.8. \url{https://CRAN.R-project.org/package=splines2}


# Appendix A: Parameterisations on the hazard scale

When `basehaz` is set equal to `"exp"`, `"weibull"`, `"gompertz"`, `"ms"` (the default), or `"bs"` then the model is defined on the hazard scale using the following parameterisations.


### Exponential model

The exponential model is parameterised with scale parameter $\lambda_i = \exp(\eta_i)$ where $\eta_i = \beta_0 + \sum_{p=1}^P \beta_p x_{ip}$ denotes our linear predictor.

For individual $i$ we have:

\begin{align}
\begin{split}
  h_i(T_i) 
    & = \lambda_i \\
    & = \exp(\eta_i) \\
  H_i(T_i) 
    & = T_i \lambda_i \\
    & = T_i \exp(\eta_i) \\
  S_i(T_i) 
    & = \exp \left( - T_i \lambda_i \right) \\
    & = \exp \left( - T_i \exp(\eta_i) \right) \\
  F_i(T_i) 
    & = 1 - \exp \left( - T_i \lambda_i \right) \\
    & = 1 - \exp \left( - T_i \exp(\eta_i) \right) \\
  S_i(T_i) - S_i(T_i^U) 
    & = \exp \left( - T_i \lambda_i \right) - \exp \left( - T_i^U \lambda_i \right) \\
    & = \exp \left( - T_i \exp(\eta_i) \right) - \exp \left( - T_i^U \exp(\eta_i) \right)
\end{split}
\end{align}

or on the log scale:

\begin{align}
\begin{split}
  \log h_i(T_i) 
    & = \log \lambda_i \\
    & = \eta_i \\
  \log H_i(T_i) 
    & = \log(T_i) + \log \lambda_i \\
    & = \log(T_i) + \eta_i \\
  \log S_i(T_i) 
    & = - T_i \lambda_i \\
    & = - T_i \exp(\eta_i) \\
  \log F_i(T_i) 
    & = \log \left( 1 - \exp \left( - T_i \lambda_i \right) \right) \\
    & = \log \left( 1 - \exp \left( - T_i \exp(\eta_i) \right) \right) \\
  \log (S_i(T_i) - S_i(T_i^U)) 
    & = \log \left[ \exp \left( - T_i \lambda_i \right) - \exp \left( - T_i^U \lambda_i \right) \right] \\
    & = \log \left[ \exp \left( - T_i \exp(\eta_i) \right) - \exp \left( - T_i^U \exp(\eta_i) \right) \right]
\end{split}
\end{align}

The definition of $\lambda$ for the baseline is:

\begin{align}
\begin{split}
  \lambda_0 = \exp(\beta_0) \Longleftrightarrow \beta_0 = \log(\lambda_0)
\end{split}
\end{align}  
  

### Weibull model

The Weibull model is parameterised with scale parameter $\lambda_i = \exp(\eta_i)$ and shape parameter $\gamma > 0$ where $\eta_i = \beta_0 + \sum_{p=1}^P \beta_p x_{ip}$ denotes our linear predictor. 

For individual $i$ we have:

\begin{align}
\begin{split}
  h_i(T_i) 
    & = \gamma t^{\gamma-1} \lambda_i \\
    & = \gamma t^{\gamma-1} \exp(\eta_i) \\
  H_i(T_i) 
    & = T_i^{\gamma} \lambda_i \\
    & = T_i^{\gamma} \exp(\eta_i) \\
  S_i(T_i) 
    & = \exp \left( - T_i^{\gamma} \lambda_i \right) \\
    & = \exp \left( - T_i^{\gamma} \exp(\eta_i) \right) \\
  F_i(T_i) 
    & = 1 - \exp \left( - {(T_i)}^{\gamma} \lambda_i \right) \\
    & = 1 - \exp \left( - {(T_i)}^{\gamma} \exp(\eta_i) \right) \\
  S_i(T_i) - S_i(T_i^U) 
    & = \exp \left( - {(T_i)}^{\gamma} \lambda_i \right) - \exp \left( - {(T_i^U)}^{\gamma} \lambda_i \right) \\
    & = \exp \left( - {(T_i)}^{\gamma} \exp(\eta_i) \right) - \exp \left( - {(T_i^U)}^{\gamma} \exp(\eta_i) \right)
\end{split}
\end{align}

or on the log scale:

\begin{align}
\begin{split}
  \log h_i(T_i) 
    & = \log(\gamma) + (\gamma-1) \log(t) + \log \lambda_i \\
    & = \log(\gamma) + (\gamma-1) \log(t) + \eta_i \\
  \log H_i(T_i) 
    & = \gamma \log(T_i) + \log \lambda_i \\
    & = \gamma \log(T_i) + \eta_i \\
  \log S_i(T_i) 
    & = - T_i^{\gamma} \lambda_i \\
    & = - T_i^{\gamma} \exp(\eta_i) \\
  \log F_i(T_i) 
    & = \log \left( 1 - \exp \left( - {(T_i)}^{\gamma} \lambda_i \right) \right) \\
    & = \log \left( 1 - \exp \left( - {(T_i)}^{\gamma} \exp(\eta_i) \right) \right) \\
  \log (S_i(T_i) - S_i(T_i^U)) 
    & = \log \left[ \exp \left( - {(T_i)}^{\gamma} \lambda_i \right) - \exp \left( - {(T_i^U)}^{\gamma} \lambda_i \right) \right] \\
    & = \log \left[ \exp \left( - {(T_i)}^{\gamma} \exp(\eta_i) \right) - \exp \left( - {(T_i^U)}^{\gamma} \exp(\eta_i) \right) \right]
\end{split}
\end{align}

The definition of $\lambda$ for the baseline is:

\begin{align}
\begin{split}
  \lambda_0 = \exp(\beta_0) \Longleftrightarrow \beta_0 = \log(\lambda_0)
\end{split}
\end{align}


### Gompertz model

The Gompertz model is parameterised with shape parameter $\lambda_i = \exp(\eta_i)$ and scale parameter $\gamma > 0$ where $\eta_i = \beta_0 + \sum_{p=1}^P \beta_p x_{ip}$ denotes our linear predictor. 

For individual $i$ we have:

\begin{align}
\begin{split}
  h_i(T_i) 
    & = \exp(\gamma T_i) \lambda_i \\
    & = \exp(\gamma T_i) \exp(\eta_i) \\
  H_i(T_i) 
    & = \frac{\exp(\gamma T_i) - 1}{\gamma} \lambda_i \\
    & = \frac{\exp(\gamma T_i) - 1}{\gamma} \exp(\eta_i) \\
  S_i(T_i) 
    & = \exp \left( \frac{-(\exp(\gamma T_i) - 1)}{\gamma} \lambda_i \right) \\
    & = \exp \left( \frac{-(\exp(\gamma T_i) - 1)}{\gamma} \exp(\eta_i) \right) \\
  F_i(T_i) 
    & = 1 - \exp \left( \frac{-(\exp(\gamma T_i) - 1)}{\gamma} \lambda_i \right) \\
    & = 1 - \exp \left( \frac{-(\exp(\gamma T_i) - 1)}{\gamma} \exp(\eta_i) \right) \\
  S_i(T_i) - S_i(T_i^U) 
    & = \exp \left( \frac{-(\exp(\gamma T_i) - 1)}{\gamma} \lambda_i \right) - \exp \left( \frac{-(\exp(\gamma T_i^U) - 1)}{\gamma} \lambda_i \right) \\
    & = \exp \left( \frac{-(\exp(\gamma T_i) - 1)}{\gamma} \exp(\eta_i) \right) - \exp \left( \frac{-(\exp(\gamma T_i^U) - 1)}{\gamma} \exp(\eta_i) \right)
\end{split}
\end{align}

or on the log scale:

\begin{align}
\begin{split}
  \log h_i(T_i) 
    & = \gamma T_i + \log \lambda_i \\
    & = \gamma T_i + \eta_i \\
  \log H_i(T_i) 
    & = \log(\exp(\gamma T_i) - 1) - \log(\gamma) + \log \lambda_i \\
    & = \log(\exp(\gamma T_i) - 1) - \log(\gamma) + \eta_i \\
  \log S_i(T_i) 
    & = \frac{-(\exp(\gamma T_i) - 1)}{\gamma} \lambda_i \\
    & = \frac{-(\exp(\gamma T_i) - 1)}{\gamma} \exp(\eta_i) \\
  \log F_i(T_i) 
    & = \log \left( 1 - \exp \left( \frac{-(\exp(\gamma T_i) - 1)}{\gamma} \lambda_i \right) \right) \\
    & = \log \left( 1 - \exp \left( \frac{-(\exp(\gamma T_i) - 1)}{\gamma} \exp(\eta_i) \right) \right) \\
  \log (S_i(T_i) - S_i(T_i^U)) 
    & = \log \left[ \exp \left( \frac{-(\exp(\gamma T_i) - 1)}{\gamma} \lambda_i \right) - \exp \left( \frac{-(\exp(\gamma T_i^U) - 1)}{\gamma} \lambda_i \right) \right] \\
    & = \log \left[ \exp \left( \frac{-(\exp(\gamma T_i) - 1)}{\gamma} \exp(\eta_i) \right) - \exp \left( \frac{-(\exp(\gamma T_i^U) - 1)}{\gamma} \exp(\eta_i) \right) \right]
\end{split}
\end{align}

The definition of $\lambda$ for the baseline is:

\begin{align}
\begin{split}
  \lambda_0 = \exp(\beta_0) \Longleftrightarrow \beta_0 = \log(\lambda_0)
\end{split}
\end{align}


### M-spline model

The M-spline model is parameterised with vector of regression coefficients $\boldsymbol{\theta} > 0$ for the baseline hazard and with covariate effects introduced through a linear predictor $\eta_i = \sum_{p=1}^P \beta_p x_{ip}$. Note that there is no intercept in the linear predictor since it is absorbed into the baseline hazard spline function.

For individual $i$ we have:

\begin{align}
\begin{split}
  h_i(T_i) 
    & = M(T_i; \boldsymbol{\theta}, \boldsymbol{k_0}) \exp(\eta_i) \\
  H_i(T_i) 
    & = I(T_i; \boldsymbol{\theta}, \boldsymbol{k_0}) \exp(\eta_i) \\
  S_i(T_i) 
    & = \exp \left( - I(T_i; \boldsymbol{\theta}, \boldsymbol{k_0}) \exp(\eta_i) \right) \\
  F_i(T_i) 
    & = 1 - \exp \left( - I(T_i; \boldsymbol{\theta}, \boldsymbol{k_0}) \exp(\eta_i) \right) \\
  S_i(T_i) - S_i(T_i^U) 
    & = \exp \left( - I(T_i; \boldsymbol{\theta}, \boldsymbol{k_0}) \exp(\eta_i) \right) - \exp \left( - I(T_i^U; \boldsymbol{\theta}, \boldsymbol{k_0}) \exp(\eta_i) \right)
\end{split}
\end{align}

or on the log scale:

\begin{align}
\begin{split}
  \log h_i(T_i) 
    & = \log(M(T_i; \boldsymbol{\theta}, \boldsymbol{k_0})) + \eta_i \\
  \log H_i(T_i) 
    & = \log(I(T_i; \boldsymbol{\theta}, \boldsymbol{k_0})) + \eta_i \\
  \log S_i(T_i) 
    & = - I(T_i; \boldsymbol{\theta}, \boldsymbol{k_0}) \exp(\eta_i) \\
  \log F_i(T_i) 
    & = \log \left[ 1 - \exp \left( - I(T_i; \boldsymbol{\theta}, \boldsymbol{k_0}) \exp(\eta_i) \right) \right] \\
  \log (S_i(T_i) - S_i(T_i^U)) 
    & = \log \left[ \exp \left( - I(T_i; \boldsymbol{\theta}, \boldsymbol{k_0}) \exp(\eta_i) \right) - \exp \left( - I(T_i^U; \boldsymbol{\theta}, \boldsymbol{k_0}) \exp(\eta_i) \right) \right]
\end{split}
\end{align}

where $M(t; \boldsymbol{\theta}, \boldsymbol{k_0})$ denotes a cubic M-spline function evaluated at time $t$ with regression coefficients $\boldsymbol{\theta}$ and basis evaluated using the vector of knot locations $\boldsymbol{k_0})$. Similarly, $I(t; \boldsymbol{\theta}, \boldsymbol{k_0})$ denotes a cubic I-spline function (i.e. integral of an M-spline) evaluated at time $t$ with regression coefficients $\boldsymbol{\theta}$ and basis evaluated using the vector of knot locations $\boldsymbol{k_0}$.


### B-spline model

The B-spline model is parameterised with vector of regression coefficients $\boldsymbol{\theta}$ and linear predictor where $\eta_i = \sum_{p=1}^P \beta_p x_{ip}$ denotes our linear predictor. Note that there is no intercept in the linear predictor since it is absorbed into the spline function.


For individual $i$ we have:

\begin{align}
\begin{split}
  h_i(T_i) 
    & = \exp \left( B(T_i; \boldsymbol{\theta}, \boldsymbol{k_0}) + \eta_i \right)
\end{split}
\end{align}

or on the log scale:

\begin{align}
\begin{split}
  \log h_i(T_i) 
    & = B(T_i; \boldsymbol{\theta}, \boldsymbol{k_0}) + \eta_i
\end{split}
\end{align}

The cumulative hazard, survival function, and CDF for the B-spline model cannot be calculated analytically. Instead, the model is only defined analytically on the hazard scale and quadrature is used to evaluate the following:

\begin{align}
\begin{split}
  H_i(T_i) 
    & = \int_0^{T_i} h_i(u) du \\
  S_i(T_i) 
    & = \exp \left( - \int_0^{T_i} h_i(u) du \right) \\
  F_i(T_i) 
    & = 1 - \exp \left( - \int_0^{T_i} h_i(u) du \right) \\
  S_i(T_i) - S_i(T_i^U) 
    & = \exp \left( -\int_0^{T_i} h_i(u) du \right) - \exp \left( - \int_0^{T_i^U} h_i(u) du \right)
\end{split}
\end{align}


### Extension to time-varying coefficients (i.e. non-proportional hazards)

We can extend the previous model formulations to allow for time-varying coefficients (i.e. non-proportional hazards). The time-varying linear predictor is introduced on the hazard scale. That is, $\eta_i$ in our previous model definitions is instead replaced by $\eta_i(t)$. This leads to an analytical form for the hazard and log hazard. However, in general, there is no longer a closed form expression for the cumulative hazard, survival function, or CDF. Therefore, when the linear predictor includes time-varying coefficients, quadrature is used to evaluate the following:

\begin{align}
\begin{split}
  H_i(T_i) 
    & = \int_0^{T_i} h_i(u) du \\
  S_i(T_i) 
    & = \exp \left( - \int_0^{T_i} h_i(u) du \right) \\
  F_i(T_i) 
    & = 1 - \exp \left( - \int_0^{T_i} h_i(u) du \right) \\
  S_i(T_i) - S_i(T_i^U) 
    & = \exp \left( -\int_0^{T_i} h_i(u) du \right) - \exp \left( - \int_0^{T_i^U} h_i(u) du \right)
\end{split}
\end{align}


# Appendix B: Parameterisations under accelerated failure times

When `basehaz` is set equal to `"exp-aft"`, or `"weibull-aft"` then the model is defined on the accelerated failure time scale using the following parameterisations.


### Exponential model

The exponential model is parameterised with scale parameter $\lambda_i = \exp(-\eta_i)$ where $\eta_i = \beta_0^* + \sum_{p=1}^P \beta_p^* x_{ip}$ denotes our linear predictor.

For individual $i$ we have:

\begin{align}
\begin{split}
  h_i(T_i) 
    & = \lambda_i \\
    & = \exp(-\eta_i) \\
  H_i(T_i) 
    & = T_i \lambda_i \\ 
    & = T_i \exp(-\eta_i) \\
  S_i(T_i) 
    & = \exp \left( - T_i \lambda_i \right) \\
    & = \exp \left( - T_i \exp(-\eta_i) \right) \\
  F_i(T_i) 
    & = 1 - \exp \left( - T_i \lambda_i \right) \\
    & = 1 - \exp \left( - T_i \exp(-\eta_i) \right) \\
  S_i(T_i) - S_i(T_i^U) 
    & = \exp \left( - T_i \lambda_i \right) - \exp \left( - T_i^U \lambda_i \right) \\
    & = \exp \left( - T_i \exp(-\eta_i) \right) - \exp \left( - T_i^U \exp(-\eta_i) \right)
\end{split}
\end{align}

or on the log scale:

\begin{align}
\begin{split}
  \log h_i(T_i) 
    & = \log \lambda_i \\
    & = -\eta_i \\
  \log H_i(T_i) 
    & = \log(T_i) + \log \lambda_i \\
    & = \log(T_i) - \eta_i \\
  \log S_i(T_i) 
    & = - T_i \lambda_i \\
    & = - T_i \exp(-\eta_i) \\
  \log F_i(T_i) 
    & = \log \left( 1 - \exp \left( - T_i \lambda_i \right) \right) \\
    & = \log \left( 1 - \exp \left( - T_i \exp(-\eta_i) \right) \right) \\
  \log (S_i(T_i) - S_i(T_i^U)) 
    & = \log \left[ \exp \left( - T_i \lambda_i) \right) - \exp \left( - T_i^U \lambda_i \right) \right] \\
    & = \log \left[ \exp \left( - T_i \exp(-\eta_i) \right) - \exp \left( - T_i^U \exp(-\eta_i) \right) \right]
\end{split}
\end{align}

The definition of $\lambda$ for the baseline is:

\begin{align}
\begin{split}
  \lambda_0 = \exp(-\beta_0^*) \Longleftrightarrow \beta_0^* = -\log(\lambda_0)
\end{split}
\end{align}  
  
The relationship between coefficients under the PH (unstarred) and AFT (starred) parameterisations are as follows:

\begin{align}
\begin{split}
  \beta_0 & = -\beta_0^* \\
  \beta_p & = -\beta_p^*
\end{split}
\end{align}

Lastly, the general form for the hazard function and survival function under an AFT model with acceleration factor $\exp(-\eta_i)$ can be used to derive the exponential AFT model defined here by setting $h_0(t) = 1$, $S_0(t) = \exp(-T_i)$, and $\lambda_i = \exp(-\eta_i)$:

\begin{align}
\begin{split}
  h_i(T_i) 
    & = \exp(-\eta_i) h_0(t \exp(-\eta_i)) \\
    & = \exp(-\eta_i) \\
    & = \lambda_i
\end{split}
\end{align}

\begin{align}
\begin{split}
  S_i(T_i) 
    & = S_0(t \exp(-\eta_i)) \\
    & = \exp(-T_i \exp(-\eta_i)) \\
    & = \exp(-T_i \lambda_i)
\end{split}
\end{align}


### Weibull model

The Weibull model is parameterised with scale parameter $\lambda_i = \exp(-\gamma \eta_i)$ and shape parameter $\gamma > 0$ where $\eta_i = \beta_0^* + \sum_{p=1}^P \beta_p^* x_{ip}$ denotes our linear predictor.

For individual $i$ we have:

\begin{align}
\begin{split}
  h_i(T_i) 
    & = \gamma t^{\gamma-1} \lambda_i \\
    & = \gamma t^{\gamma-1} \exp(-\gamma \eta_i) \\
  H_i(T_i) 
    & = T_i^{\gamma} \lambda_i \\
    & = T_i^{\gamma} \exp(-\gamma \eta_i) \\
  S_i(T_i) 
    & = \exp \left( - T_i^{\gamma} \lambda_i \right) \\
    & = \exp \left( - T_i^{\gamma} \exp(-\gamma \eta_i) \right) \\
  F_i(T_i) 
    & = 1 - \exp \left( - {(T_i)}^{\gamma} \lambda_i \right) \\
    & = 1 - \exp \left( - {(T_i)}^{\gamma} \exp(-\gamma \eta_i) \right) \\
  S_i(T_i) - S_i(T_i^U) 
    & = \exp \left( - {(T_i)}^{\gamma} \lambda_i \right) - \exp \left( - {(T_i^U)}^{\gamma} \lambda_i \right) \\
    & = \exp \left( - {(T_i)}^{\gamma} \exp(-\gamma \eta_i) \right) - \exp \left( - {(T_i^U)}^{\gamma} \exp(-\gamma \eta_i) \right)
\end{split}
\end{align}

or on the log scale:

\begin{align}
\begin{split}
  \log h_i(T_i) 
    & = \log(\gamma) + (\gamma-1) \log(t) + \log \lambda_i \\
    & = \log(\gamma) + (\gamma-1) \log(t) - \gamma \eta_i \\
  \log H_i(T_i) 
    & = \gamma \log(T_i) + \log \lambda_i \\
    & = \gamma \log(T_i) - \gamma \eta_i \\
  \log S_i(T_i) 
    & = - T_i^{\gamma} \lambda_i \\
    & = - T_i^{\gamma} \exp(-\gamma \eta_i) \\
  \log F_i(T_i) 
    & = \log \left( 1 - \exp \left( - {(T_i)}^{\gamma} \lambda_i \right) \right) \\
    & = \log \left( 1 - \exp \left( - {(T_i)}^{\gamma} \exp(-\gamma \eta_i) \right) \right) \\
  \log (S_i(T_i) - S_i(T_i^U)) 
    & = \log \left[ \exp \left( - {(T_i)}^{\gamma} \lambda_i \right) - \exp \left( - {(T_i^U)}^{\gamma} \lambda_i \right) \right] \\
    & = \log \left[ \exp \left( - {(T_i)}^{\gamma} \exp(-\gamma \eta_i) \right) - \exp \left( - {(T_i^U)}^{\gamma} \exp(-\gamma \eta_i) \right) \right]
\end{split}
\end{align}

The definition of $\lambda$ for the baseline is:

\begin{align}
\begin{split}
  \lambda_0 = \exp(-\gamma \beta_0^*) \Longleftrightarrow \beta_0^* = \frac{-\log(\lambda_0)}{\gamma}
\end{split}
\end{align}  
  
The relationship between coefficients under the PH (unstarred) and AFT (starred) parameterisations are as follows:

\begin{align}
\begin{split}
  \beta_0 & = -\gamma \beta_0^* \\
  \beta_p & = -\gamma \beta_p^*  
\end{split}
\end{align}

Lastly, the general form for the hazard function and survival function under an AFT model with acceleration factor $\exp(-\eta_i)$ can be used to derive the Weibull AFT model defined here by setting $h_0(t) = \gamma t^{\gamma - 1}$, $S_0(t) = \exp(-T_i^{\gamma})$, and $\lambda_i = \exp(-\gamma \eta_i)$:

\begin{align}
\begin{split}
  h_i(T_i) 
    & = \exp(-\eta_i) h_0(t \exp(-\eta_i)) \\
    & = \exp(-\eta_i) \gamma {(t \exp(-\eta_i))}^{\gamma - 1} \\
    & = \exp(-\gamma \eta_i) \gamma t^{\gamma - 1} \\
    & = \lambda_i \gamma t^{\gamma - 1}
\end{split}
\end{align}

\begin{align}
\begin{split}
  S_i(T_i) 
    & = S_0(t \exp(-\eta_i)) \\
    & = \exp(-(T_i \exp(-\eta_i))^{\gamma}) \\
    & = \exp(-T_i^{\gamma} [\exp(-\eta_i)]^{\gamma}) \\
    & = \exp(-T_i^{\gamma} \exp(-\gamma \eta_i)) \\
    & = \exp(-T_i \lambda_i)
\end{split}
\end{align}


### Extension to time-varying coefficients (i.e. time-varying acceleration factors)

We can extend the previous model formulations to allow for time-varying coefficients (i.e. time-varying acceleration factors). 

The so-called "unmoderated" survival probability for an individual at time $t$ is defined as the baseline survival probability at time $t$, i.e. $S_i(t) = S_0(t)$. With a time-fixed acceleration factor, the survival probability for a so-called "moderated" individual is defined as the baseline survival probability but evaluated at "time $t$ multiplied by the acceleration factor $\exp(-\eta_i)$". That is, the survival probability for the moderated individual is $S_i(t) = S_0(t \exp(-\eta_i))$. 

However, with time-varying acceleration we cannot simply multiply time by a fixed (acceleration) constant. Instead, we must integrate the function for the time-varying acceleration factor over the interval $0$ to $t$. In other words, we must evaluate:
\
\begin{align}
\begin{split}
  S_i(t) = S_0 \left( \int_0^t \exp(-\eta_i(u)) du \right)
\end{split}
\end{align}
\
as described by Hougaard (1999).

Hougaard also gives a general expression for the hazard function under time-varying acceleration, as follows:
\
\begin{align}
\begin{split}
  h_i(t) = \exp \left(-\eta_i(t) \right) h_0 \left( \int_0^t \exp(-\eta_i(u)) du \right)
\end{split}
\end{align}

**Note:** It is interesting to note here that the *hazard* at time $t$ is in fact a function of the full history of covariates and parameters (i.e. the linear predictor) from time $0$ up until time $t$. This is different to the hazard scale formulation of time-varying effects (i.e. non-proportional hazards). Under the hazard scale formulation with time-varying effects, the *survival* probability is a function of the full history between times $0$ and $t$, but the *hazard* is **not**; instead, the hazard is only a function of covariates and parameters as defined at the current time. This is particularly important to consider when fitting accelerated failure time models with time-varying effects in the presence of delayed entry (i.e. left truncation).

For the exponential distribution, this leads to:

\begin{align}
\begin{split}
  S_i(T_i) 
    & = S_0 \left( \int_0^{T_i} \exp(-\eta_i(u)) du \right) \\
    & = \exp \left(- \int_0^{T_i} \exp(-\eta_i(u)) du \right)
\end{split}
\end{align}

\begin{align}
\begin{split}
  h_i(T_i) 
    & = \exp \left(-\eta_i(T_i) \right) h_0 \left( \int_0^{T_i} \exp(-\eta_i(u)) du \right) \\
    & = \exp \left(-\eta_i(T_i) \right) \exp \left(- \int_0^{T_i} \exp(-\eta_i(u)) du \right)
\end{split}
\end{align}

and for the Weibull distribution, this leads to:

\begin{align}
\begin{split}
  S_i(T_i) 
    & = S_0 \left( \int_0^{T_i} \exp(-\eta_i(u)) du \right) \\
    & = \exp \left(- \left[\int_0^{T_i} \exp (-\eta_i(u)) du \right]^{\gamma} \right)
\end{split}
\end{align}

\begin{align}
\begin{split}
  h_i(T_i) 
    & = \exp \left(-\eta_i(T_i) \right) h_0 \left( \int_0^{T_i} \exp(-\eta_i(u)) du \right) \\
    & = \exp \left(-\eta_i(T_i) \right) \exp \left(- \left[\int_0^{T_i} \exp (-\eta_i(u)) du \right]^{\gamma} \right)
\end{split}
\end{align}

The general expressions for the hazard and survival function under an AFT model with a time-varying linear predictor are used to evaluate the likelihood for the accelerated failure time model in `stan_surv` when time-varying effects are specified in the model formula. Specifically, quadrature is used to evaluate the cumulative acceleration factor $\int_0^t \exp(-\eta_i(u)) du$ and this is then substituted into the relevant expressions for the hazard and survival.